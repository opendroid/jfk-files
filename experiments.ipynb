{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF and image experiments\n",
    "Leverages PyMuPDF, Image and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import easyocr, torch\n",
    "import pytesseract\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import io\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "from Levenshtein import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF file: /Users/xyz-ai/Developer/data/jfk/docs/pdf/20250318/198-10007-10029.pdf\n",
      "Number of pages: 16\n",
      "creator: Aspose Pty Ltd.\n",
      "producer: Aspose.PDF for .NET 23.2.0\n",
      "creationDate: 2025-03-18 03:49:29\n",
      "modifiedDate: 2025-03-18 03:49:29\n"
     ]
    }
   ],
   "source": [
    "# Read config\n",
    "with open(\"config.yml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Extract images from PDFs\n",
    "pdf_file = config[\"sample_pdf\"][\"doc1\"]\n",
    "\n",
    "# Read PDF\n",
    "doc = pymupdf.open(pdf_file)\n",
    "print(f\"PDF file: {pdf_file}\")\n",
    "print(f\"Number of pages: {doc.page_count}\")\n",
    "print(f\"creator: {doc.metadata['creator']}\")\n",
    "print(f\"producer: {doc.metadata['producer']}\")\n",
    "date_format = \"%Y%m%d%H%M%S\"\n",
    "creation_date = doc.metadata['creationDate'][2:16]\n",
    "creation_str = datetime.strptime(creation_date, date_format)\n",
    "modified_date = doc.metadata['modDate'][2:16]\n",
    "modified_str = datetime.strptime(modified_date, date_format)\n",
    "print(f\"creationDate: {creation_str}\")\n",
    "print(f\"modifiedDate: {modified_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The get_page_images(pno, full=False) returns followings:\n",
    "\n",
    " - xref (int) is the image object number\n",
    " - smask (int) is the object number of its soft-mask image\n",
    " - width (int) is the image width\n",
    " - height (int) is the image height\n",
    " - bpc (int) denotes the number of bits per component (normally 8)\n",
    " - colorspace (str) a string naming the colorspace (like DeviceRGB)\n",
    " - alt_colorspace (str) is any alternate colorspace depending on the value of colorspace\n",
    " - name (str) is the symbolic name by which the image is referenced\n",
    " - filter (str) is the decode filter of the image (Adobe PDF References, pp. 22).\n",
    " - referencer (int) the xref of the referencer. Zero if directly referenced by the page. Only present if full=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of pages in the PDF\n",
    "page_number = 0\n",
    "for page in doc:\n",
    "    page_number += 1\n",
    "    image_count = 0\n",
    "    images = page.get_images()\n",
    "    for image in images:\n",
    "        image_count += 1\n",
    "        xref = image[0] # Reference number of the image object\n",
    "        w, h = image[2:4] # width and height of the image\n",
    "        bw = image[4] # bits per pixel, 1 is black and white\n",
    "        print(f\"Page: {page_number}, Images: {image_count}: {xref} of {w}x{h}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the first page images\n",
    "page1_images = doc[0].get_images()\n",
    "for image in page1_images:\n",
    "    print(image)\n",
    "first_image_xref = page1_images[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first image and show it\n",
    "pix = pymupdf.Pixmap(doc, first_image_xref)  # create a Pixmap\n",
    "img = Image.open(io.BytesIO(pix.tobytes(\"png\")))\n",
    "#display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tesseract\n",
    "Use the [pytesseract](https://pypi.org/project/pytesseract/) to perform the OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform OCR on the image, using pytesseract\n",
    "text_pytesseract = pytesseract.image_to_string(img)\n",
    "print(text_pytesseract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EasyOCR\n",
    "[EasyOCR: A Comprehensive Guide](https://medium.com/@adityamahajan.work/easyocr-a-comprehensive-guide-5ff1cb850168) is a easy guide for OCR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform OCR on the image, using easyocr\n",
    "# Set device to MPS if available, else fallback to CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize EasyOCR reader\n",
    "reader = easyocr.Reader(['en'], gpu=True) # gpu=True for MPS\n",
    "reader.detector.to(device) # Move detection model to MPS\n",
    "reader.recognizer.to(device) # Move recognition model to MPS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and process image\n",
    "# direct PNG file: image_path = config[\"sample_image\"][\"page1\"]\n",
    "if isinstance(img, str):\n",
    "    image_path = img\n",
    "else:\n",
    "    image_path = np.array(img)\n",
    "\n",
    "text_easyocr = reader.readtext(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "for (bbox, text, prob) in text_easyocr:\n",
    "    print(f\"{text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Microsoft TrOCR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"pooler_act\": \"tanh\",\n",
      "  \"pooler_output_size\": 768,\n",
      "  \"qkv_bias\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.0\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load TrOCR processor and model\n",
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\n",
    "model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\n",
    "\n",
    "# Optional: Move to GPU (CUDA or MPS) if available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available(\n",
    ") else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the image\n",
    "image_path = config[\"sample_image\"][\"page1\"]\n",
    "# Load the image in grayscale, better for text recognition\n",
    "image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "image = cv2.convertScaleAbs(image, alpha=1.5, beta=10)\n",
    "image_denoised = cv2.fastNlMeansDenoising(image, h=20)\n",
    "_, image_binary = cv2.threshold(image_denoised, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "kernel = np.ones((2, 2), np.uint8)  # Smaller kernel to avoid over-connection\n",
    "image_binary = cv2.dilate(image_binary, kernel, iterations=1)  # Reduced iterations\n",
    "\n",
    "# Pytesseract detection\n",
    "custom_config = r'--psm 11 --oem 3 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789.,:- -c min_characters_to_try=3'\n",
    "boxes = pytesseract.image_to_data(image_binary, output_type=pytesseract.Output.DICT, config=custom_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect detected regions with stricter filters\n",
    "# Collect detected regions\n",
    "detected_regions = []\n",
    "for i in range(len(boxes['text'])):\n",
    "    text = boxes['text'][i].strip()\n",
    "    if text and len(text) >= 3:\n",
    "        x, y, w, h = (boxes['left'][i], boxes['top'][i],\n",
    "                      boxes['width'][i], boxes['height'][i])\n",
    "        if w > 30 and h > 8:\n",
    "            detected_regions.append((x, y, w, h))\n",
    "\n",
    "# Contour detection\n",
    "contours, _ = cv2.findContours(\n",
    "    image_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "for contour in contours:\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    if w > 30 and h > 8 and not any(abs(r[0] - x) < 15 and abs(r[1] - y) < 15 for r in detected_regions):\n",
    "        detected_regions.append((x, y, w, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge overlapping regions\n",
    "def merge_regions(regions):\n",
    "    merged = []\n",
    "    regions = sorted(regions, key=lambda r: (r[1], r[0]))  # Sort by y, then x\n",
    "    while regions:\n",
    "        x1, y1, w1, h1 = regions.pop(0)\n",
    "        i = 0\n",
    "        while i < len(regions):\n",
    "            x2, y2, w2, h2 = regions[i]\n",
    "            if (abs(y1 - y2) < 15 and  # Vertical overlap/close\n",
    "                    max(x1, x2) < min(x1 + w1, x2 + w2) + 20):  # Horizontal overlap with buffer\n",
    "                w1 = max(x1 + w1, x2 + w2) - min(x1, x2)\n",
    "                h1 = max(h1, h2)\n",
    "                x1 = min(x1, x2)\n",
    "                y1 = min(y1, y2)\n",
    "                regions.pop(i)\n",
    "            else:\n",
    "                i += 1\n",
    "        merged.append((x1, y1, w1, h1))\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_regions = merge_regions(detected_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_texts = set()\n",
    "text_trocr = []\n",
    "for (x, y, w, h) in merged_regions:\n",
    "    line_image = image[y:y+h, x:x+w]\n",
    "    line_pil = Image.fromarray(line_image).convert('RGB')\n",
    "    pixel_values = processor(line_pil, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    generated_ids = model.generate(pixel_values, num_beams=5, max_length=50)\n",
    "    trocr_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "    if len(trocr_text) >= 1:\n",
    "        seen_texts.add(trocr_text)\n",
    "        text_trocr.append(trocr_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2125 RELEASE UNDER THE PRESIDENT JOHN F. KENNEDY ASSASSNATION RECORDS ACT OF 1992\n",
      "198-10007-10029\n",
      "JFK ASSASSINATION SYSTEM\n",
      "DATE:\n",
      "6/24/201\n",
      "IDENTIFICATION FORM\n",
      "AGENCY INFORMATION\n",
      "AGENCY\n",
      "ARMY\n",
      "RECORD NUMBER\n",
      "198-10007-10029\n",
      "RECORD SERIES\n",
      "CALIFANO PAPERS\n",
      "AGENCY FILE NUMBER\n",
      "DOCUMENT INFORMATION\n",
      "ORIGINATOR\n",
      "CIA\n",
      "FROM\n",
      "C. TRACY BARNES\n",
      "REPORT ON THE COLD WAR USE OF RADIO BROADCASTING BY CLA\n",
      "TITLE\n",
      "06/25/1953\n",
      "DATE\n",
      "PAGES\n",
      "SUBJECTS\n",
      "RADIO BROADCASTING - CIA\n",
      "DOCUMENT TYPE\n",
      "PAPER, TEXTUAL DOCUMENT\n",
      "CLASSIFICATION\n",
      "SECRET\n",
      "RESTRICTIONS\n",
      "1A; 1B\n",
      "CURRENT STATUS\n",
      "REDACT\n",
      "DATE OF LAST REVIEW\n",
      "04/03/1998\n",
      "OPENING CRITERIA\n",
      "CALIFANO PAPERS, BOX 1, FOLDER 2, MEMO FROM BAMES RE: REPOT ON THE COLD WAR USE OF RATIO\n",
      "COMMENTS :\n",
      "BROADCASTING BY CIA.\n",
      "JFK RAV:UN\n",
      "JF\n",
      "K\n",
      "AVV\n",
      "AV\n",
      "DEPARTMENT OF THE ARMY EO 13526\n",
      "DELASILY EXCLUDE D\n",
      "EXEMPT\n",
      "AUTHORITY\n",
      "D REFER TO\n",
      "DATE ANGIS BY MARNGLV/6-9\n",
      "REVIEW\n",
      "WWWWWWWWWWWWWWWW\n",
      "V9.1\n",
      "NW 50955 DOCID:32424022 PAGE 1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(text_trocr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Visualize bounding boxes\n",
    "image_color = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "for (x, y, w, h) in merged_regions:\n",
    "    cv2.rectangle(image_color, (x, y), (x+w, y+h), (0, 255, 0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert image_color to PIL image\n",
    "image_color_pil = Image.fromarray(image_color)\n",
    "# display(image_color_pil)\n",
    "# Save the image\n",
    "cv2.imwrite('/Users/xyz-ai/Downloads/jfk_page1_annotated.png', image_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.version.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
